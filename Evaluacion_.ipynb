{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Evaluacion .ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djaramilloq/Djaramilloq.github.io/blob/main/Evaluacion_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOJ0-4fxU6xa"
      },
      "source": [
        "# Evaluación del curso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up5jHeZdU6xd"
      },
      "source": [
        "Use el archivo scopus-abstracts.csv (https://github.com/jdvelasq/datalabs/blob/master/datasets/scopus-abstracts.csv) para resolver los siguientes problemas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QGU58uKU6xe"
      },
      "source": [
        "## Parte 1: Librería `TextBlob`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RifvslQiU6xf"
      },
      "source": [
        "**1.--** Cuente la cantidad de sustantivos, adjetivos, verbos y advervios presentes en el texto usando TextBlob. Preprocese el texto. Esto es, por ejemplo,  llevar las conjugaciones a su infinitivo, llevar las palabras de su plural a su singular, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwA9eqD1U6xf",
        "outputId": "e20e8527-2b77-4749-eab0-48557d87ff71"
      },
      "source": [
        "# importa pandas\n",
        "import pandas as pd\n",
        "\n",
        "# importar libreria nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')\n",
        "\n",
        "# Lee el archivo desde el repo en GitHub\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/scopus-abstracts.csv\", encoding='latin-1')\n",
        "\n",
        "# Convierte los abstracts a una lista de texto\n",
        "text = df.Abstract.tolist()\n",
        "\n",
        "# Une los strings para formar una sola cadena de texto\n",
        "text = ' '.join(text)\n",
        "\n",
        "# Importa el texto a TextBlob \n",
        "# lo transforma en minusculas\n",
        "from textblob import TextBlob\n",
        "text = TextBlob(text)\n",
        "text = text.lower()\n",
        "\n",
        "# Crea un diccionario para realiar el conteo e inicia \n",
        "# los contadores en cero\n",
        "counter_dict = {}\n",
        "counter_dict['NN'] = 0\n",
        "counter_dict['JJ'] = 0\n",
        "counter_dict['VB'] = 0\n",
        "counter_dict['RB'] = 0\n",
        "\n",
        "# Recorre las parejas de palabras - tags y los cuenta usando los tipos de tags\n",
        "for word, tag in text.tags:\n",
        "    if tag[0:2] == 'NN':\n",
        "        # un sustantivo\n",
        "        counter_dict['NN'] += 1\n",
        "    if tag[:2] == 'JJ':\n",
        "        # un adjetivo\n",
        "        counter_dict['JJ'] += 1\n",
        "    if tag[:2] == 'VB':\n",
        "        # un verbo\n",
        "        counter_dict['VB'] += 1\n",
        "    if tag[:2] == 'RB':\n",
        "        # un adverbio\n",
        "        counter_dict['RB'] += 1\n",
        "for key in counter_dict.keys():\n",
        "    print(key, counter_dict[key])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "NN 110915\n",
            "JJ 40977\n",
            "VB 47673\n",
            "RB 9605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdDOVN0FU6xh"
      },
      "source": [
        "**2.--** Liste las 10 frases nominales (noun phrases) más comunes en el texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCu7hIAwU6xh"
      },
      "source": [
        "# importa pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Lee el archivo desde el repo en GitHub\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/scopus-abstracts.csv\", encoding='latin-1')\n",
        "\n",
        "# Convierte los abstracts a una lista de texto\n",
        "text = df.Abstract.tolist()\n",
        "\n",
        "# Une los strings para formar una sola cadena de texto\n",
        "text = ' '.join(text)\n",
        "\n",
        "# Importa el texto a TextBlob \n",
        "# lo transforma en minusculas\n",
        "from textblob import TextBlob\n",
        "text = TextBlob(text)\n",
        "text = text.lower()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjhQ7I18U6xi",
        "outputId": "1c7519f8-336c-4e38-f1f0-8c92acc315ba"
      },
      "source": [
        "# Obtiene la lista de frases nominales\n",
        "noun_phrases = text.noun_phrases\n",
        "\n",
        "# crea un diccionario para contar la ocurrencia de cada frase\n",
        "counts = {}\n",
        "for noun_phrase in noun_phrases:\n",
        "    counts[noun_phrase] = str(text).count(noun_phrase)\n",
        "counts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fundamental requirements': 1,\n",
              " 'human life': 6,\n",
              " 'significant societal impacts': 1,\n",
              " 'human movements': 3,\n",
              " 'specific patterns': 1,\n",
              " 'normal periods': 1,\n",
              " 'such patterns': 2,\n",
              " 'extreme events': 3,\n",
              " 'extreme event': 6,\n",
              " 'mobility resilience': 3,\n",
              " 'mobility system': 2,\n",
              " 'manage shocks': 1,\n",
              " 'steady state': 1,\n",
              " 'detect extreme events': 1,\n",
              " 'movement data': 3,\n",
              " 'transient loss': 1,\n",
              " 'measure resilience metrics': 1,\n",
              " 'social media data': 18,\n",
              " 'multiple types': 2,\n",
              " 'higher-order socio-economic impacts': 1,\n",
              " 'resilient infrastructures': 1,\n",
              " 'nationâ\\x80\\x99s overall disaster resilience strategies': 1,\n",
              " 'recent rise': 1,\n",
              " 'political extremism': 2,\n",
              " 'western countries': 2,\n",
              " 'moral appeal': 2,\n",
              " 'empirical support': 2,\n",
              " 'psychological explanation': 1,\n",
              " 'extremist groups': 1,\n",
              " 'field studies': 1,\n",
              " 'psychological measures': 1,\n",
              " 'contrast groups': 1,\n",
              " 'u.s. context': 1,\n",
              " 'twitter data': 9,\n",
              " 'political extremists': 3,\n",
              " 'psychological constructs': 1,\n",
              " 'conservative users': 1,\n",
              " 'positive emotion': 3,\n",
              " 'negative emotion': 1,\n",
              " 'partisan users': 2,\n",
              " 'extremists express': 2,\n",
              " 'language indicative': 1,\n",
              " 'moral foundations theory': 2,\n",
              " 'ingroup loyalty': 1,\n",
              " 'moral foundations': 3,\n",
              " 'informational landscape': 2,\n",
              " 'democratic access': 1,\n",
              " 'information outlets': 1,\n",
              " 'news outlets': 4,\n",
              " 'traditional ones': 1,\n",
              " 'chilean media ecosystem': 1,\n",
              " 'gravity model': 5,\n",
              " 'analyze geography': 1,\n",
              " 'audience reachability': 1,\n",
              " 'geographical factors': 1,\n",
              " 'local news outlets': 1,\n",
              " 'national-distribution outlets': 1,\n",
              " 'regression model': 40,\n",
              " 'political characteristics': 1,\n",
              " 'news outlets adoption': 1,\n",
              " 'national-distribution news outlets target populations': 1,\n",
              " 'influential nodes': 3,\n",
              " 'fundamental issue': 2,\n",
              " 'wide applications': 3,\n",
              " 'information diffusion': 4,\n",
              " 'network topology': 5,\n",
              " 'identify influential nodes': 1,\n",
              " 'eigenvalue centrality': 1,\n",
              " 'real-world networks': 2,\n",
              " 'so-called modular structure': 1,\n",
              " 'recent works': 2,\n",
              " 'significant effect': 5,\n",
              " 'modular network': 2,\n",
              " 'local influence': 2,\n",
              " 'intra-community links': 1,\n",
              " 'global influence': 2,\n",
              " 'inter-community links': 1,\n",
              " 'community structure': 17,\n",
              " 'standard centrality measures': 1,\n",
              " 'modular networks': 1,\n",
              " 'so-called â\\x80\\x9cmodular centralityâ\\x80\\x9d': 1,\n",
              " 'two-dimensional vector': 1,\n",
              " 'component quantifies': 2,\n",
              " 'modular centrality extensions': 1,\n",
              " 'scalar counterparts': 1,\n",
              " 'epidemic process setting': 1,\n",
              " 'simulation results': 14,\n",
              " 'synthetic networks': 1,\n",
              " 'clear idea': 1,\n",
              " 'major type': 1,\n",
              " 'traditional dietary surveys': 1,\n",
              " 'digital data': 6,\n",
              " 'peopleâ\\x80\\x99s health': 2,\n",
              " 'online studies': 1,\n",
              " 'regional level': 1,\n",
              " 'food consumption': 3,\n",
              " 'loyalty cards': 1,\n",
              " 'main grocery retailer': 1,\n",
              " 'health outcomes': 5,\n",
              " 'publicly-available medical prescription records': 1,\n",
              " 'general practitioners': 3,\n",
              " 'analyze 1.6b food item': 1,\n",
              " 'medical prescriptions': 1,\n",
              " 'entire city': 3,\n",
              " 'nutrient diversity': 1,\n",
              " 'â\\x80\\x9cmetabolic syndromeâ\\x80\\x9d': 1,\n",
              " 'high cholesterol': 1,\n",
              " 'rich world': 1,\n",
              " 'linear regression models': 3,\n",
              " 'census areas': 1,\n",
              " 'healthy areas': 2,\n",
              " '% accuracy': 3,\n",
              " 'income matters': 1,\n",
              " 'diversify nutrients': 1,\n",
              " 'large quantities': 7,\n",
              " 'study shows': 10,\n",
              " 'digital records': 2,\n",
              " 'scalable tool': 1,\n",
              " 'health surveillance': 3,\n",
              " 'different stakeholders': 3,\n",
              " 'insurance companies': 2,\n",
              " 'food companies': 1,\n",
              " 'effective prevention strategies': 1,\n",
              " 'past decades': 2,\n",
              " 'soft skills': 8,\n",
              " 'labour market outcomes': 1,\n",
              " 'labour market inequality': 1,\n",
              " 'previous research shows': 1,\n",
              " 'work explores': 1,\n",
              " 'job advertisements': 1,\n",
              " 'computational science': 1,\n",
              " 'empirical insights': 1,\n",
              " 'semi-automatic approach': 1,\n",
              " 'crucial component': 1,\n",
              " 'job ads': 1,\n",
              " 'low-paid jobs': 1,\n",
              " 'work shows': 8,\n",
              " 'partial predictors': 1,\n",
              " 'gender composition': 2,\n",
              " 'job categories': 1,\n",
              " 'equal wage returns': 1,\n",
              " 'labour market': 5,\n",
              " 'â\\x80\\x9cfemaleâ\\x80\\x9d skills': 1,\n",
              " 'wage penalties': 1,\n",
              " 'wage inequality': 1,\n",
              " 'occupational gender segregation': 1,\n",
              " 'labour markets': 1,\n",
              " 'gender-specific preference': 2,\n",
              " 'potential mate choice': 1,\n",
              " 'usersâ\\x80\\x99 behavioral data': 1,\n",
              " 'large online': 4,\n",
              " 'network measures': 3,\n",
              " 'menâ\\x80\\x99s attributes': 2,\n",
              " 'own requirements': 2,\n",
              " 'mate choice': 2,\n",
              " 'own attributes': 1,\n",
              " 'menâ\\x80\\x99s requirements': 1,\n",
              " 'womenâ\\x80\\x99s attributes': 1,\n",
              " 'women attach': 1,\n",
              " 'great importance': 3,\n",
              " 'socio-economic status': 7,\n",
              " 'potential partners': 1,\n",
              " 'own socio-economic status': 2,\n",
              " 'potential mates': 1,\n",
              " 'ensemble learning classification methods': 1,\n",
              " 'centrality indices': 2,\n",
              " 'important factors': 4,\n",
              " 'correlation analysis': 6,\n",
              " 'different strategic behaviors': 1,\n",
              " 'positive correlation': 7,\n",
              " 'recommendation engines': 1,\n",
              " 'potential dates': 1,\n",
              " 'new avenues': 3,\n",
              " 'data-driven research': 1,\n",
              " 'strategic behavior': 3,\n",
              " 'game theory': 2,\n",
              " 'economy platforms': 2,\n",
              " 'fair marketplaces': 1,\n",
              " 'large-scale online aggregators': 1,\n",
              " 'offline human biases': 1,\n",
              " 'easy-to-access digital spaces': 1,\n",
              " 'equal opportunities': 1,\n",
              " 'such platforms': 2,\n",
              " 'discriminatory behaviours': 1,\n",
              " 'gentrification phenomena': 1,\n",
              " 'racial lines': 1,\n",
              " 'economy platform': 3,\n",
              " 'user base': 3,\n",
              " 'large cities': 12,\n",
              " 'racial composition': 3,\n",
              " 'statistical analysis': 12,\n",
              " 'quantify behaviours': 1,\n",
              " 'airbnb hosts': 1,\n",
              " 'property types': 1,\n",
              " 'such behaviours': 1,\n",
              " 'platform design recommendations': 1,\n",
              " 'inclusive growth': 1,\n",
              " 'mutual information': 2,\n",
              " 'anticipation relationships': 1,\n",
              " 'financial traders': 2,\n",
              " 'non-professional investors': 1,\n",
              " 'private investment firm': 1,\n",
              " 'different assets': 1,\n",
              " 'spanish ibex market': 1,\n",
              " 'peculiar topology': 1,\n",
              " 'random networks': 1,\n",
              " 'alternative features': 1,\n",
              " 'human behavior': 24,\n",
              " 'synchronization links': 1,\n",
              " 'anticipation links': 1,\n",
              " 'one-day delay': 1,\n",
              " 'significant role': 8,\n",
              " 'network structure': 12,\n",
              " 'individuals reaction': 1,\n",
              " 'price changes': 3,\n",
              " 'synchronization network': 1,\n",
              " 'significant effects': 4,\n",
              " 'anticipation network': 1,\n",
              " 'prediction accuracy': 8,\n",
              " 'random forest models': 1,\n",
              " 'individual investors': 1,\n",
              " 'human relations individualsâ\\x80\\x99 gender': 1,\n",
              " 'age play': 3,\n",
              " 'key role': 7,\n",
              " 'social arrangements': 1,\n",
              " 'gender preferences': 3,\n",
              " 'different stages': 5,\n",
              " 'large mobile phone dataset': 1,\n",
              " 'callee combinations': 1,\n",
              " 'human interactions': 2,\n",
              " 'different levels': 23,\n",
              " 'wide scope': 1,\n",
              " 'human sociality': 1,\n",
              " 'relative strength': 4,\n",
              " 'detail records': 7,\n",
              " 'analysis suggests': 2,\n",
              " 'strong age dependence': 1,\n",
              " 'opposite gender': 3,\n",
              " 'reproductive age': 2,\n",
              " 'strong tendency': 1,\n",
              " 'possible interactions': 1,\n",
              " 'long duration': 1,\n",
              " 'gender interactions': 2,\n",
              " 'reproductive years': 1,\n",
              " 'potential emotional exchange': 1,\n",
              " 'whereas fathers': 1,\n",
              " 'generation contacts': 1,\n",
              " 'null model': 2,\n",
              " 'cohesive groups': 1,\n",
              " 'urban spaces': 2,\n",
              " 'fundamental process': 1,\n",
              " 'social structure': 5,\n",
              " 'large-scale data': 21,\n",
              " 'mobility patterns': 13,\n",
              " 'urban social groups': 1,\n",
              " 'mobile phone dataset': 2,\n",
              " 'usual voice': 1,\n",
              " 'text message': 3,\n",
              " 'internet activity information': 1,\n",
              " 'mobile subscribers': 1,\n",
              " 'metropolitan area': 11,\n",
              " 'social groups interact': 1,\n",
              " 'urban space': 7,\n",
              " 'extensive analysis': 2,\n",
              " 'study concern': 1,\n",
              " 'social group behavior': 1,\n",
              " 'visit patterns': 2,\n",
              " 'urban groups': 2,\n",
              " 'interaction patterns shows': 1,\n",
              " 'urban groups need': 1,\n",
              " 'frequent on-phone interactions': 1,\n",
              " 'such locations': 1,\n",
              " 'preferences impact': 1,\n",
              " 'group get-togethers': 1,\n",
              " 'smart transportation infrastructure': 1,\n",
              " 'traffic congestion': 4,\n",
              " 'weak signal': 1,\n",
              " 'city landscape': 1,\n",
              " 'small percentage': 1,\n",
              " 'population uses': 1,\n",
              " 'traditional techniques': 3,\n",
              " 'households travel diaries': 1,\n",
              " 'traditional approaches': 6,\n",
              " 'vehicle availability': 2,\n",
              " 'european cities': 2,\n",
              " 'major active car': 1,\n",
              " 'urban activity indicators': 1,\n",
              " 'drop-off events': 1,\n",
              " 'spatio-temporal information': 1,\n",
              " 'different zones': 1,\n",
              " 'direct application': 1,\n",
              " 'maintenance facilities': 1,\n",
              " 'operation area': 1,\n",
              " 'notable origin': 1,\n",
              " 'interpersonal relations': 1,\n",
              " 'efficient tool': 2,\n",
              " 'huge number': 8,\n",
              " 'challenging targets': 1,\n",
              " 'different faiths': 1,\n",
              " 'different religions': 2,\n",
              " 'major factor': 3,\n",
              " 'social conflicts': 1,\n",
              " 'quantitative understanding': 1,\n",
              " 'religious segregation': 2,\n",
              " 'social network': 129,\n",
              " 'religion network': 1,\n",
              " 'comparative analysis shows': 1,\n",
              " 'different political parties': 1,\n",
              " 'cross-religion links': 1,\n",
              " 'charitable issues': 1,\n",
              " 'quantitative insights': 1,\n",
              " 'valuable evidence': 1,\n",
              " 'religious syncretism': 1,\n",
              " 'global community decorates': 1,\n",
              " 'personal decisions': 1,\n",
              " 'contextual influences': 1,\n",
              " 'economic surroundings': 1,\n",
              " 'spatial patterns': 10,\n",
              " 'residential decoration practices': 1,\n",
              " 'private nature': 1,\n",
              " 'interior home spaces': 1,\n",
              " 'geographic culture hearths and/or globalization trends': 1,\n",
              " 'interior living spaces': 1,\n",
              " 'popular home': 1,\n",
              " 'rental website': 1,\n",
              " 'learning techniques': 16,\n",
              " 'key stylistic objects': 1,\n",
              " 'wall art': 1,\n",
              " 'vibrant colors': 1,\n",
              " 'decor practices': 1,\n",
              " 'deep dive': 1,\n",
              " 'major u.s. cities': 1,\n",
              " 'world regions': 1,\n",
              " 'significant variation': 4,\n",
              " 'decorative element prevalence': 1,\n",
              " 'cultural trends': 1,\n",
              " 'u.s. neighborhood level': 1,\n",
              " 'socio-economic neighborhood variables': 1,\n",
              " 'unemployment rates': 2,\n",
              " 'education attainment': 1,\n",
              " 'residential property value': 1,\n",
              " 'racial diversity': 2,\n",
              " 'american residents': 1,\n",
              " 'different socio-economic environments': 1,\n",
              " 'similar effort': 1,\n",
              " 'new view': 1,\n",
              " 'worldwide human behavior': 1,\n",
              " 'new application': 2,\n",
              " 'machine learning techniques': 13,\n",
              " 'cultural phenomena': 1,\n",
              " 'new information': 5,\n",
              " 'communication technologiesâ\\x80\\x94social media': 1,\n",
              " 'political activism': 1,\n",
              " 'recent decades': 4,\n",
              " 'spatiotemporal learning approach': 1,\n",
              " 'social movement theories': 1,\n",
              " 'deep learning framework': 1,\n",
              " 'geographical contexts': 2,\n",
              " 'social media discussions': 1,\n",
              " 'novel predictive framework': 1,\n",
              " 'new design': 5,\n",
              " 'attentional networks': 1,\n",
              " 'spatiotemporal structure': 1,\n",
              " 'future protests': 1,\n",
              " 'theory-relevant interpretationsâ\\x80\\x94it': 1,\n",
              " 'significant contributions': 1,\n",
              " 'experiment results': 7,\n",
              " 'movement events': 1,\n",
              " 'interesting comparisons': 1,\n",
              " 'recent movements': 1,\n",
              " 'mercalli intensity scale': 2,\n",
              " 'mercalli scale': 8,\n",
              " 'qualitative measure': 1,\n",
              " 'accurate intensity reports': 1,\n",
              " 'emergency response': 2,\n",
              " 'particular earthquake': 1,\n",
              " 'mercalli scale reports': 4,\n",
              " 'possible consequences': 1,\n",
              " 'strong earthquakes': 1,\n",
              " 'previous events': 1,\n",
              " 'emergency offices': 1,\n",
              " 'seismological agencies worldwide': 1,\n",
              " 'task relies': 1,\n",
              " 'human observers': 1,\n",
              " 'early prediction': 2,\n",
              " 'spatial mercalli scale reports': 1,\n",
              " 'peopleâ\\x80\\x99s reactions': 1,\n",
              " 'social networks': 61,\n",
              " 'usersâ\\x80\\x99 comments': 1,\n",
              " 'real-time earthquakes': 1,\n",
              " 'mercalli scale point estimates': 2,\n",
              " 'state subdivisions': 1,\n",
              " 'level granularity': 1,\n",
              " 'mercalli support': 1,\n",
              " 'â\\x80\\x98local supportâ\\x80\\x99': 1,\n",
              " 'mercalli scale estimates': 1,\n",
              " 'real-world events': 2,\n",
              " 'smooth point estimates': 1,\n",
              " 'social media': 127,\n",
              " 'spatial reports': 1,\n",
              " 'experimental results': 89,\n",
              " 'spatial mercalli reports': 1,\n",
              " 'method performs': 2,\n",
              " 'earthquake spatial detection': 1,\n",
              " 'maximum intensity prediction tasks': 1,\n",
              " 'valuable source': 3,\n",
              " 'spatial information': 11,\n",
              " 'earthquake damages': 1,\n",
              " 'pattern detection': 2,\n",
              " 'network models': 4,\n",
              " 'global structure': 1,\n",
              " 'local node interactions': 1,\n",
              " 'migration flow networks': 1,\n",
              " 'sociologic trends': 1,\n",
              " 'global settings': 1,\n",
              " 'topo-algebraic methods': 1,\n",
              " 'global patterns': 2,\n",
              " 'simultaneous interactions': 1,\n",
              " 'multiple nodes': 1,\n",
              " 'holistic perspective': 1,\n",
              " 'network fabric': 2,\n",
              " 'order description': 1,\n",
              " 'overall flow structure': 1,\n",
              " 'asian net migration': 1,\n",
              " 'remittance networks': 1,\n",
              " 'clique complexes': 1,\n",
              " 'specific flow patterns': 1,\n",
              " 'generate diagrams': 1,\n",
              " 'migrant movement patterns': 1,\n",
              " 'big data architectures': 2,\n",
              " 'recent years': 39,\n",
              " 'twitter uses stream processing frameworks': 1,\n",
              " 'apache storm': 2,\n",
              " 'analyse billions': 1,\n",
              " 'big data': 577,\n",
              " 'different components': 2,\n",
              " 'different connectors': 1,\n",
              " 'such complex architectures': 1,\n",
              " 'difficult task': 9,\n",
              " 'software architects': 1,\n",
              " 'initial designs': 1,\n",
              " 'ordinary static topology inference analysis': 1,\n",
              " 'common anti-patterns': 1,\n",
              " 'software verification techniques': 1,\n",
              " 'architectural models': 2,\n",
              " 'paper illustrates ostia': 1,\n",
              " 'industrial-scale case-studies': 1,\n",
              " 'data validation': 1,\n",
              " 'organisations update': 1,\n",
              " 'data transformations': 2,\n",
              " 'new version': 1,\n",
              " 'correct output': 1,\n",
              " 'validation tool': 2,\n",
              " 'tool compares': 1,\n",
              " 'tabular databases': 1,\n",
              " 'different versions': 1,\n",
              " 'database comparison': 1,\n",
              " 'accurate results': 6,\n",
              " 'test scenarios': 1,\n",
              " 'human error': 2,\n",
              " 'manual alternative': 1,\n",
              " 'turnaround time': 1,\n",
              " 'validation process': 2,\n",
              " 'agile way': 1,\n",
              " 'data transformation workflows': 1,\n",
              " 'sick days': 1,\n",
              " 'clock tireless work': 1,\n",
              " 'artificial intelligence': 11,\n",
              " 'exciting future': 1,\n",
              " 'future development': 8,\n",
              " 'real game changer': 1,\n",
              " 'commercial availability': 1,\n",
              " 'class citizens': 1,\n",
              " 'processing power': 1,\n",
              " 'enable future computers': 1,\n",
              " 'incredible amounts': 2,\n",
              " 'jobâ\\x80\\x99s human': 1,\n",
              " 'quantum supremacy': 1,\n",
              " 'humans grasp': 1,\n",
              " 'brain computer interface': 1,\n",
              " 'ai security': 1,\n",
              " 'â\\x80\\x9ccan ai': 1,\n",
              " 'autonomous entityâ\\x80\\x9d': 1,\n",
              " 'ethical use': 1,\n",
              " 'human race': 1,\n",
              " 'inevitable path': 1,\n",
              " 'ai dominance': 1,\n",
              " 'â\\x80\\x9cwill humans': 1,\n",
              " 'powerful tool': 9,\n",
              " 'big datasets': 12,\n",
              " 'temporal datasets': 4,\n",
              " 'additional parameter': 1,\n",
              " 'algorithmic formulation': 1,\n",
              " 'such data': 31,\n",
              " 'health data': 10,\n",
              " 'certain temporal range': 1,\n",
              " 'cases real-time processing': 1,\n",
              " 'issues increase algorithmic complexity': 1,\n",
              " 'processing times': 4,\n",
              " 'storage requirements': 2,\n",
              " 'process confidential data': 1,\n",
              " 'public clusters': 1,\n",
              " 'optimise algorithms': 1,\n",
              " 'standalone systems': 1,\n",
              " 'efficient codes': 1,\n",
              " 'infrequent events': 1,\n",
              " 'new scheme': 2,\n",
              " 'time-series data storage': 1,\n",
              " 'multicore parallelisation': 1,\n",
              " 'new algorithms': 3,\n",
              " 'fast robust pattern': 1,\n",
              " 'wide range': 28,\n",
              " 'new formulation': 1,\n",
              " 'temporal restrictions': 1,\n",
              " 'possible extensions': 1,\n",
              " 'modern standalone multicore workstation': 1,\n",
              " 'security issues': 6,\n",
              " 'confidential data': 4,\n",
              " 'available weather dataset': 1,\n",
              " 'confidential adult': 1,\n",
              " 'social care dataset': 1,\n",
              " 'previous algorithms': 1,\n",
              " 'spam algorithm': 1,\n",
              " 'sequential pattern': 11,\n",
              " 'time points': 1,\n",
              " 'algorithm outperforms spam': 1,\n",
              " 'new algorithm': 18,\n",
              " 'large amount': 38,\n",
              " 'spatiotemporal data': 1,\n",
              " 'location-capture technologies': 1,\n",
              " 'mobile devices': 11,\n",
              " 'data needs': 3,\n",
              " 'raw trajectory data': 1,\n",
              " 'useful knowledge': 9,\n",
              " 'traffic insights': 1,\n",
              " 'stage preprocesses': 1,\n",
              " 'stage applies': 1,\n",
              " 'processing time': 16,\n",
              " 'comparative analysis': 13,\n",
              " 'silhouette coefficient shows': 1,\n",
              " 'outperforms hdbscan': 1,\n",
              " 'clusters quality': 1,\n",
              " 'analysis shows': 7,\n",
              " 'rs outperforms k-means': 1,\n",
              " 'square error': 2,\n",
              " 'google maps approach': 1,\n",
              " 'traffic districts': 1,\n",
              " 'extract taxi trips flow': 1,\n",
              " 'sequential patterns': 7,\n",
              " 'different areas': 7,\n",
              " 'visualize traffic clusters': 1,\n",
              " 'rs algorithm': 1,\n",
              " 'taxi trips heatmap': 1,\n",
              " 'porto city': 1,\n",
              " 'current traffic control applications': 1,\n",
              " 'useful guidelines': 1,\n",
              " 'taxi drivers': 1,\n",
              " 'transportation authorities': 1,\n",
              " 'python projects': 2,\n",
              " 'public software repositories': 1,\n",
              " 'git pull': 1,\n",
              " 'deliberate cognitive activities': 1,\n",
              " 'large corpus': 1,\n",
              " 'source code': 4,\n",
              " 'python repositories': 1,\n",
              " 'git activity data': 1,\n",
              " 'initial work': 2,\n",
              " 'primary questions': 1,\n",
              " 'adoption event': 1,\n",
              " 'future adoption activity': 1,\n",
              " 'user behavior': 8,\n",
              " 'library adoptions': 1,\n",
              " 'normal activity': 1,\n",
              " 'normal cognitive effort': 1,\n",
              " 'specific types': 2,\n",
              " 'simple linear model': 1,\n",
              " 'future commits': 2,\n",
              " 'additional work': 1,\n",
              " 'new libraries': 1,\n",
              " 'big data contexts': 1,\n",
              " 'file system': 3,\n",
              " 'storage systems': 3,\n",
              " 'data organization strategies': 2,\n",
              " 'query performance': 9,\n",
              " 'storage technology': 1,\n",
              " 'paper evaluates': 2,\n",
              " 'different data organization strategies': 1,\n",
              " 'big data warehouses': 1,\n",
              " 'potential benefit': 5,\n",
              " 'response time': 15,\n",
              " 'intensive workload': 1,\n",
              " 'overall decreases': 1,\n",
              " 'shows potential benefits': 1,\n",
              " 'specific scenarios': 1,\n",
              " 'deep learning': 41,\n",
              " 'standard deep architectures': 1,\n",
              " 'classification problems': 9,\n",
              " 'empirical software engineering': 3,\n",
              " 'large volumes': 19,\n",
              " 'low-shot learning': 3,\n",
              " 'total number': 5,\n",
              " 'uml class': 1,\n",
              " 'sequence diagrams': 1,\n",
              " 'good performance': 6,\n",
              " 'appropriate architecture': 1,\n",
              " 'off-the-shelf architecture': 1,\n",
              " 'deep learning algorithms': 2,\n",
              " 'empirical software engineering community': 1,\n",
              " 'global popularity': 1,\n",
              " 'social media platforms': 4,\n",
              " 'unprecedented amounts': 1,\n",
              " 'affective states': 1,\n",
              " 'individual users': 4,\n",
              " 'systematic explorations': 1,\n",
              " 'large datasets': 25,\n",
              " 'valuable information': 3,\n",
              " 'sociocultural variables': 1,\n",
              " 'global nature': 1,\n",
              " 'present unique methodological': 1,\n",
              " 'yield findings': 1,\n",
              " 'specific sociocultural context': 1,\n",
              " 'large social media datasets': 1,\n",
              " 'arab world': 1,\n",
              " 'social media use': 9,\n",
              " 'arab emirates': 1,\n",
              " 'large social media dataset': 2,\n",
              " 'twitter messages': 4,\n",
              " 'numerous differences': 1,\n",
              " 'significant variations': 3,\n",
              " 'users networks': 1,\n",
              " 'sentiment analytic tools': 1,\n",
              " 'word graphs': 1,\n",
              " 'temporal patterns': 6,\n",
              " 'positive sentiment': 5,\n",
              " 'happiest hour': 1,\n",
              " 'happiest day': 1,\n",
              " 'happy day': 1,\n",
              " 'happiest months': 1,\n",
              " 'sentiment patterns': 1,\n",
              " 'religio-cultural significance': 1,\n",
              " 'alzheimerâ\\x80\\x99s disease': 3,\n",
              " 'mild cognitive impairment': 1,\n",
              " 'early diagnosis': 2,\n",
              " 'high dimension': 23,\n",
              " 'neural data': 1,\n",
              " 'available samples': 1,\n",
              " 'precise computer diagnostic system': 1,\n",
              " 'useful tool': 5,\n",
              " 'raw data': 10,\n",
              " 'two-stage method': 2,\n",
              " 'intelligent diagnosis': 1,\n",
              " 'uncontrolled two-layer neural network': 1,\n",
              " 'softmax regression': 1,\n",
              " 'categorize health statuses': 1,\n",
              " 'alzheimerâ\\x80\\x99s brain images': 1,\n",
              " 'good diagnostic accuracy': 1,\n",
              " 'brain image data': 1,\n",
              " 'method reduces': 1,\n",
              " 'human work': 1,\n",
              " 'big data processing': 9,\n",
              " 'multi-class classification': 2,\n",
              " 'ad/mci diagnosis': 1,\n",
              " 'crucial elements': 1,\n",
              " 'medical applications': 3,\n",
              " 'open platforms': 1,\n",
              " 'traditional model': 3,\n",
              " 'personal information': 8,\n",
              " 'privacy layer': 1,\n",
              " 'mr layer': 1,\n",
              " 'smr model': 2,\n",
              " 'core benefit': 1,\n",
              " 'model creates': 1,\n",
              " 'scalability issues': 2,\n",
              " 'privacy-utility tradeoff': 1,\n",
              " 'data miners': 2,\n",
              " 'information loss': 4,\n",
              " 'remarkable improvement': 1,\n",
              " 'memory usage': 3,\n",
              " 'crowd density': 11,\n",
              " 'indoor dance events': 1,\n",
              " 'new method': 35,\n",
              " 'indoor wi-fi localization': 1,\n",
              " 'smart phones': 3,\n",
              " 'probabilistic model': 3,\n",
              " 'statistical mechanics': 2,\n",
              " 'big data analytics': 63,\n",
              " 'machine learning': 121,\n",
              " 'mac address randomization': 1,\n",
              " 'packet interarrival times': 1,\n",
              " 'main result': 3,\n",
              " 'crowd size increases': 1,\n",
              " 'dangerous crowd density': 1,\n",
              " 'conceptual simplicity': 1,\n",
              " 'non-parametric mixture models': 1,\n",
              " 'identify latent clusters': 1,\n",
              " 'landsat imagery': 1,\n",
              " 'such model': 6,\n",
              " 'fit bayesian non-parametric models': 1,\n",
              " 'computational time': 3,\n",
              " 'useful information': 16,\n",
              " 'identify clusters': 1,\n",
              " 'low interest': 1,\n",
              " 'landsat images': 1,\n",
              " 'brisbane region': 1,\n",
              " 'actual sources': 1,\n",
              " 'fire-ant eradication program': 1,\n",
              " 'australian states': 1,\n",
              " 'federal government': 4,\n",
              " 'budgetary constraints': 1,\n",
              " 'fire-ant incursion': 1,\n",
              " 'eradication program focuses': 1,\n",
              " 'high risk clusters': 1,\n",
              " 'fieldwork survey data': 1,\n",
              " 'deep learning techniques': 1,\n",
              " 'effective classification': 1,\n",
              " 'important area': 6,\n",
              " 'high class imbalance': 1,\n",
              " 'real-world applications': 7,\n",
              " 'fraud detection': 11,\n",
              " 'cancer detection': 2,\n",
              " 'data poses': 2,\n",
              " 'exhibit bias': 1,\n",
              " 'majority class': 2,\n",
              " 'extreme cases': 1,\n",
              " 'minority class altogether': 1,\n",
              " 'class imbalance': 24,\n",
              " 'traditional machine learning models': 1,\n",
              " 'non-deep learning': 1,\n",
              " 'recent advances': 6,\n",
              " 'empirical work': 1,\n",
              " 'class imbalance exists': 1,\n",
              " 'performance results': 2,\n",
              " 'complex domains': 1,\n",
              " 'deep neural networks': 2,\n",
              " 'high levels': 4,\n",
              " 'great interest': 3,\n",
              " 'available studies': 1,\n",
              " 'survey discusses': 1,\n",
              " 'implementation details': 2,\n",
              " 'additional insight': 2,\n",
              " 'data complexity': 2,\n",
              " 'performance interpretation': 1,\n",
              " 'big data application': 14,\n",
              " 'work focuses': 4,\n",
              " 'computer vision tasks': 1,\n",
              " 'convolutional neural networks': 7,\n",
              " 'traditional methods': 13,\n",
              " 'cost-sensitive learning': 1,\n",
              " 'neural network': 75,\n",
              " 'learning abilities': 1,\n",
              " 'promising results': 2,\n",
              " 'survey concludes': 1,\n",
              " 'various gaps': 1,\n",
              " 'future research': 29,\n",
              " 'customer churn': 4,\n",
              " 'major problem': 6,\n",
              " 'large companies': 2,\n",
              " 'direct effect': 2,\n",
              " 'telecom field': 1,\n",
              " 'potential customer': 1,\n",
              " 'increase customer churn': 1,\n",
              " 'necessary actions': 1,\n",
              " 'main contribution': 12,\n",
              " 'churn prediction model': 2,\n",
              " 'assists telecom operators': 1,\n",
              " 'likely subject': 1,\n",
              " 'work uses machine learning techniques': 1,\n",
              " 'big data platform': 9,\n",
              " 'new way': 4,\n",
              " 'featuresâ\\x80\\x99 engineering': 1,\n",
              " 'standard measure': 1,\n",
              " 'auc value': 2,\n",
              " 'prediction model': 16,\n",
              " 'social network analysis': 6,\n",
              " 'large dataset': 33,\n",
              " 'big raw data': 1,\n",
              " 'syriatel telecom company': 2,\n",
              " 'customersâ\\x80\\x99 information': 1,\n",
              " 'decision tree': 35,\n",
              " 'random forest': 29,\n",
              " 'machine tree â\\x80\\x9cgbmâ\\x80\\x9d': 1,\n",
              " 'extreme gradient': 1,\n",
              " 'xgboost algorithm': 1,\n",
              " 'churn predictive model': 1,\n",
              " 'hospital readmissions': 3,\n",
              " 'significant cost savings': 1,\n",
              " 'readmission analysis frameworks': 1,\n",
              " 'data type': 9,\n",
              " 'data size': 10,\n",
              " 'disease conditions': 1,\n",
              " 'large amounts': 24,\n",
              " 'readmission prediction analysis': 1,\n",
              " 'available patient data': 1,\n",
              " 'readmission risk analysis': 2,\n",
              " 'high dimensionality': 4,\n",
              " 'new data': 24,\n",
              " 'continuous basis': 1,\n",
              " 'prediction power': 2,\n",
              " 'risk models': 2,\n",
              " 'high performance': 25,\n",
              " 'big data readmission risk analysis framework': 1,\n",
              " 'uses nave bayes classification algorithm': 1,\n",
              " 'over-all evaluation time': 1,\n",
              " 'model performance': 7,\n",
              " 'critical systems': 1,\n",
              " 'big data streams': 4,\n",
              " 'human operators': 2,\n",
              " 'event streams': 3,\n",
              " 'â\\x80\\x98human-in-the-loopâ\\x80\\x99 operator': 1,\n",
              " 'suitable training data': 1,\n",
              " 'correct course': 1,\n",
              " 'real-time events': 1,\n",
              " 'visual depiction': 1,\n",
              " 'event data': 7,\n",
              " 'essential associations': 1,\n",
              " 'visual inspection': 1,\n",
              " 'similar requirement': 2,\n",
              " 'activity protocols': 1,\n",
              " 'large organization': 1,\n",
              " 'activity traces': 2,\n",
              " 'minimal delay': 1,\n",
              " 'adaptive window': 1,\n",
              " 'network diagrams': 1,\n",
              " 'event label co-occurrence networks': 1,\n",
              " 'intuitive method': 2,\n",
              " 'network construction': 1,\n",
              " 'complex event streams': 1,\n",
              " 'facilitates creation': 1,\n",
              " 'smart spaces': 1,\n",
              " 'sensor-rich data-centric cyber-physical systems': 1,\n",
              " 'industry 4.0.': 2,\n",
              " 'commercial/industrial contexts': 1,\n",
              " 'everyday life': 4,\n",
              " 'way people access': 2,\n",
              " 'healthcare services': 4,\n",
              " 'healthcare industry': 8,\n",
              " 'iot systems': 1,\n",
              " 'smart healthcare spaces': 1,\n",
              " 'extreme amounts': 1,\n",
              " 'valuable insights': 9,\n",
              " 'paper focuses': 14,\n",
              " 'smart healthcare domain': 1,\n",
              " 'data fusion': 7,\n",
              " 'iot networks': 1,\n",
              " 'edge devices': 1,\n",
              " 'communications units': 1,\n",
              " 'cloud platforms': 1,\n",
              " 'hierarchical data fusion architecture': 1,\n",
              " 'different data sources': 4,\n",
              " 'iot taxonomy': 1,\n",
              " 'mission-critical decisions': 1,\n",
              " 'smart healthcare scenario': 1,\n",
              " 'minimum time delay': 1,\n",
              " 'necessary information': 1,\n",
              " 'complex event processing technology': 1,\n",
              " 'hierarchical processing model': 1,\n",
              " 'data â\\x80\\x98on': 1,\n",
              " 'flyâ\\x80\\x99â\\x80\\x94a key requirement': 1,\n",
              " 'iot devices': 1,\n",
              " 'time-critical application domains': 1,\n",
              " 'initial experiments': 1,\n",
              " 'different data fusion levels': 1,\n",
              " 'overall performance': 3,\n",
              " 'reaction time': 2,\n",
              " 'public healthcare services': 1,\n",
              " 'iot technologies': 1,\n",
              " 'healthcare industry 4.0. â©': 1,\n",
              " 'global environmental pollution': 1,\n",
              " 'human activities': 6,\n",
              " 'public health': 18,\n",
              " 'adverse environmental conditions': 4,\n",
              " 'unique physiological': 1,\n",
              " 'behavioral characteristics': 2,\n",
              " 'childhood diseases': 1,\n",
              " 'important economic center': 1,\n",
              " 'rapid population expansion': 1,\n",
              " 'medical resources': 2,\n",
              " 'serious public health problem': 1,\n",
              " 'environmental effect': 1,\n",
              " 'overall pediatric admissions': 2,\n",
              " 'multi-center study': 1,\n",
              " 'pediatric admission data': 1,\n",
              " 'overall pediatric admissions/day': 1,\n",
              " 'tertiary pediatric hospitals': 1,\n",
              " 'large-scale health information exchange network': 1,\n",
              " 'admission data': 3,\n",
              " 'local environmental data': 1,\n",
              " 'seasonal decomposition method': 1,\n",
              " 'time-trend analysis': 1,\n",
              " 'additive model': 4,\n",
              " 'environmental measurements': 1,\n",
              " 'admissions data': 1,\n",
              " 'emergency departments': 1,\n",
              " 'calendar factors': 1,\n",
              " 'different clinical departments': 1,\n",
              " 'nitrogen dioxide': 1,\n",
              " '% increase': 2,\n",
              " '% confidence interval': 2,\n",
              " 'outpatient admissions': 2,\n",
              " '% ci': 2,\n",
              " 'emergency admissions': 2,\n",
              " 'fine particles â\\x89¤': 1,\n",
              " 'carbon monoxide': 1,\n",
              " 'pediatric admissions': 4,\n",
              " 'co concentrations': 1,\n",
              " 'current-day outpatient admissions': 1,\n",
              " 'current-day emergency admissions': 1,\n",
              " 'health information exchange network': 2,\n",
              " 'multi-center retrospective study': 1,\n",
              " 'study contributes': 3,\n",
              " 'environmental health research': 1,\n",
              " 'pediatric resource planning': 1,\n",
              " 'efficient method': 7,\n",
              " 'product aspects': 1,\n",
              " 'customer reviews': 2,\n",
              " 'aspect ratings': 3,\n",
              " 'aspect weights': 2,\n",
              " 'userâ\\x80\\x99s satisfaction': 1,\n",
              " 'aspect weights reflect': 1,\n",
              " 'important role': 29,\n",
              " 'customersâ\\x80\\x99 opinion': 1,\n",
              " 'study addresses': 2,\n",
              " 'aspect extraction': 1,\n",
              " 'aspect words': 2,\n",
              " 'conditional probability': 2,\n",
              " 'bootstrap technique': 1,\n",
              " 'naã¯ve bayes classification method': 1,\n",
              " 'sentiment words': 3,\n",
              " 'aspect consistency': 1,\n",
              " 'method obtains': 1,\n",
              " 'real world datasets': 4,\n",
              " 'state-of-the-art methods': 8,\n",
              " 'states healthcare system': 1,\n",
              " 'enormous volume': 1,\n",
              " 'vast number': 2,\n",
              " 'financial transactions': 2,\n",
              " 'healthcare fraud': 1,\n",
              " 'fraudulent transactions': 1,\n",
              " 'detect fraudulent activities': 1,\n",
              " 'monetary losses': 1,\n",
              " 'â\\x80\\x98big dataâ\\x80\\x99 medicare claims datasets': 1,\n",
              " 'real-world fraudulent physicians': 1,\n",
              " 'test dataset': 3,\n",
              " 'medicare parts': 2,\n",
              " 'assess fraud detection performance': 1,\n",
              " 'emulate class rarity': 1,\n",
              " 'severe levels': 1,\n",
              " 'additional datasets': 1,\n",
              " 'fraud instances': 1,\n",
              " 'fraud detection performance': 4,\n",
              " 'machine learning model': 10,\n",
              " 'real-world use': 3,\n",
              " 'performance evaluation': 8,\n",
              " 'error rates': 4,\n",
              " 'detection rates': 1,\n",
              " 'severe class imbalance': 2,\n",
              " 'evaluation method': 4,\n",
              " 'machine learning results': 1,\n",
              " 'viable substitute': 1,\n",
              " 'medicare fraud detection': 2,\n",
              " 'severe class imbalance datasets': 1,\n",
              " 'fraudulent instances': 1,\n",
              " 'assess potential improvements': 1,\n",
              " 'adverse effects': 3,\n",
              " 'train_test method': 1,\n",
              " 'outperforms cross-validation': 1,\n",
              " 'tree ensembles': 1,\n",
              " 'representative model': 2,\n",
              " 'multiple decision-tree models': 1,\n",
              " 'different models': 2,\n",
              " 'entire dataset': 1,\n",
              " 'different similarity metrics': 1,\n",
              " 'linear combination': 3,\n",
              " 'tree selection methodology': 1,\n",
              " 'popular ensemble algorithm': 1,\n",
              " 'local models': 1,\n",
              " 'alternative tree selection strategies': 1,\n",
              " 'validation accuracy': 2,\n",
              " 'original ensemble': 1,\n",
              " 'representative trees': 1,\n",
              " 'comparative evaluation experiments': 1,\n",
              " 'popular decision-tree algorithms': 1,\n",
              " 'different amounts': 1,\n",
              " 'equal-size slices': 1,\n",
              " 'syntactic similarity approach': 1,\n",
              " 'sysmâ\\x80\\x94syntactic similarity method': 1,\n",
              " 'significant difference': 10,\n",
              " 'ensemble algorithms': 1,\n",
              " 'representative models': 1,\n",
              " 'big data classification': 3,\n",
              " 'long time': 3,\n",
              " 'data variety': 3,\n",
              " 'approximation approaches suffer': 1,\n",
              " 'poor accuracy': 1,\n",
              " 'various parts': 1,\n",
              " 'different impact': 4,\n",
              " 'final result': 2,\n",
              " 'aware approximation approach': 1,\n",
              " 'acceptable error': 1,\n",
              " 'confidence interval': 6,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdJi7isAU6xk",
        "outputId": "2d5c4da6-31af-4b7f-8bf7-4224febb1075"
      },
      "source": [
        "# crea una lista ordenada como en los ejemplos de clase usando itemgetter\n",
        "from operator import itemgetter\n",
        "\n",
        "# Ordena el diccionario por los valores como en la clase\n",
        "counts = sorted(counts.items(), key=itemgetter(1), reverse=True)\n",
        "\n",
        "# Retorna las primeras 10 frases mas frecuentes\n",
        "counts[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('l p', 838),\n",
              " ('big data', 577),\n",
              " ('copyright â©', 279),\n",
              " ('inderscience enterprises', 233),\n",
              " ('inderscience enterprises ltd.', 232),\n",
              " ('order t', 176),\n",
              " ('social network', 129),\n",
              " ('social media', 127),\n",
              " ('machine learning', 121),\n",
              " ('time series', 114)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs2pXiOQU6xl"
      },
      "source": [
        "**3.--** Liste los cinco (5) tags más frecuentes en el texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kefiKCE_U6xm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef84f678-5eb8-41cd-fce4-695c9cc8f9c5"
      },
      "source": [
        "# importa pandas\n",
        "import pandas as pd\n",
        "\n",
        "# importa libreria de NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Lee el archivo directamente desde el repo en GitHub\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/scopus-abstracts.csv\", encoding='latin-1')\n",
        "\n",
        "# Convierte los abstracts a una lista de texto\n",
        "text = df.Abstract.tolist()\n",
        "\n",
        "# Une los strings para formar una sola cadena de texto\n",
        "text = ' '.join(text)\n",
        "\n",
        "# Importa el texto a TextBlob y lo transforma en minusculas\n",
        "from textblob import TextBlob\n",
        "text = TextBlob(text)\n",
        "text = text.lower()\n",
        "\n",
        "# crea un diccionario para el conteo\n",
        "counter_dict = {}\n",
        "\n",
        "# recorre las parejas palabra tag\n",
        "for word, tag in text.tags:\n",
        "    # si el tag no existe en el diccionario, lo crea con contador igual a cero\n",
        "    if tag not in counter_dict.keys():\n",
        "        counter_dict[tag] = 0\n",
        "        \n",
        "    # Le suma una unidad al tag encontrado\n",
        "    counter_dict[tag] += 1\n",
        "    \n",
        "    \n",
        "# ordena usando itemgetter como en clase\n",
        "from operator import itemgetter\n",
        "\n",
        "# Ordena el diccionario por los valores como en clase\n",
        "counts = sorted(counter_dict.items(), key=itemgetter(1), reverse=True)\n",
        "\n",
        "# Retorna los primeros cinco elementos del conteo\n",
        "counts[:5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NN', 75764), ('IN', 40887), ('JJ', 39347), ('NNS', 34935), ('DT', 33799)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0D-kiYhU6xn"
      },
      "source": [
        "## Parte 2: Librería NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jcDZSbWU6xo"
      },
      "source": [
        "**4.--** Preprocese los abstracts originales usando NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY7WwBX0U6xo",
        "outputId": "30df2729-7a84-478e-a793-7ba6f833044d"
      },
      "source": [
        "# importa libreria nltk\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXPC-M3gU6xo"
      },
      "source": [
        "# importa pandas\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/scopus-abstracts.csv\",\n",
        "    sep=\",\",\n",
        "    thousands=None,\n",
        "    decimal=\".\",\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "# remueve el copyright como en el ejemplo\n",
        "# https://jdvelasq.github.io/mineria-de-datos/notebooks/text-analytics/2-04-NLTK-operaciones-basicas-sobre-texto.html\n",
        "data[\"Abstract\"] = data.Abstract.map(\n",
        "    lambda w: w[0 : w.find(\"\\u00a9\")], na_action=\"ignore\"\n",
        ")\n",
        "\n",
        "# procesa los abstracts tal como se indicó en clase\n",
        "abstracts = data.Abstract.copy()\n",
        "abstracts = abstracts.dropna()\n",
        "abstracts = abstracts.map(lambda w: w.strip())\n",
        "abstracts = abstracts.map(lambda w: w + \".\" if w[-1] != \".\" else w)\n",
        "abstracts = abstracts.map(lambda w: w.lower(), na_action=\"ignore\")\n",
        "abstracts = abstracts.tolist()\n",
        "abstracts = \" \".join(abstracts)\n",
        "\n",
        "# separa las palabras y las convierte a un objeto de \n",
        "# texto de NLTK\n",
        "from nltk.tokenize import word_tokenize\n",
        "abstracts = word_tokenize(abstracts)\n",
        "abstracts = nltk.Text(abstracts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFYU1FVZU6xp"
      },
      "source": [
        "**5.--** Encuentre las collocations más frecuentes en el texto. Tenga en cuenta que debe eliminar collocations que no tienen sentido, por ejemplo, 'results show' es invalido, pero 'big data' es válido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1CGi9d1U6xp",
        "outputId": "0d4e71fb-590e-4d05-e3a8-0fe89b23dc8c"
      },
      "source": [
        "# El ejemplo de clase solo imprime las colocaciones\n",
        "# pero no permite manipularlas\n",
        "abstracts.collocations(num=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "big data; machine learning; social media; time series; data mining;\n",
            "results show; experimental results; case study; supply chain; decision\n",
            "making; data sets; mobile phone; paper presents; paper proposes;\n",
            "united states; land use; association rules; neural network; data\n",
            "analytics; social networks; deep learning; recent years; proposed\n",
            "method; data management; association rule; business intelligence;\n",
            "cloud computing; wide range; built environment; web pages\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wD6jecdU6xp",
        "outputId": "586421b5-d41c-47e7-8d75-1b7707e96888"
      },
      "source": [
        "# De la pagina oficial: https://www.nltk.org/_modules/nltk/text.html\n",
        "# Obtiene la lista de los 30 principales bigramas\n",
        "bigramas = abstracts.collocation_list(num=30, window_size=3)\n",
        "\n",
        "# une las tuplas que conforman el bigrama\n",
        "bigramas = [' '.join(text) for text in bigramas]\n",
        "\n",
        "# Se crea una lista de exclusión.\n",
        "exclusion = [\n",
        "    'results show',\n",
        "    'experimental results',\n",
        "    'case study',\n",
        "    'paper presents',\n",
        "    'paper proposes',\n",
        "    'recent years',\n",
        "    'proposed method',\n",
        "    'wide range',\n",
        "    'different types',\n",
        "]\n",
        "\n",
        "# Excluye el bigrama si esta en la lista de exclusion\n",
        "bigramas = [texto for texto in bigramas if texto not in exclusion]\n",
        "bigramas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['big data',\n",
              " 'machine learning',\n",
              " 'social media',\n",
              " 'time series',\n",
              " 'data mining',\n",
              " 'supply chain',\n",
              " 'decision making',\n",
              " 'mobile phone',\n",
              " 'data sets',\n",
              " 'united states',\n",
              " 'land use',\n",
              " 'association rules',\n",
              " 'neural network',\n",
              " 'social networks',\n",
              " 'big analytics',\n",
              " 'deep learning',\n",
              " 'data analytics',\n",
              " 'association rule',\n",
              " 'cloud computing',\n",
              " 'feature selection',\n",
              " 'business intelligence']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5LPfG0IU6xq"
      },
      "source": [
        "**6.--** Para las collocations en el punto 5, reemplace los espacios en blanco por '_', es decir, el texto 'big data' se transforma a 'big_data'. Use el nuevo texto (collocations) para agregar un paso adicional a la fase de preprocesamiento, tal que se reemplacen las collocations por su equivalente que usa el caracter de subrayado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_D1Y2PuU6xq",
        "outputId": "e9e3bb95-3306-4516-c6bb-dcddbd3bd7b0"
      },
      "source": [
        "# reemplaza los espacios en blanco por subrayado\n",
        "bigramas = [texto.replace(' ', '_') for texto in bigramas]\n",
        "\n",
        "# Se agrega el paso adicional\n",
        "abstracts = data.Abstract.copy()\n",
        "abstracts = abstracts.dropna()\n",
        "abstracts = abstracts.map(lambda w: w.strip())\n",
        "abstracts = abstracts.map(lambda w: w + \".\" if w[-1] != \".\" else w)\n",
        "abstracts = abstracts.map(lambda w: w.lower(), na_action=\"ignore\")\n",
        "abstracts = abstracts.tolist()\n",
        "abstracts = \" \".join(abstracts)\n",
        "\n",
        "# se reemplaza el bigrama original por el bigrama con '_'\n",
        "for bigrama in bigramas:\n",
        "    abstracts = abstracts.replace(bigrama.replace('_', ' '), bigrama)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "abstracts = word_tokenize(abstracts)\n",
        "abstracts = nltk.Text(abstracts)\n",
        "\n",
        "# para verificar se chequea el primer bigrama (big_data)\n",
        "abstracts.concordance(bigramas[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Displaying 25 of 488 matches:\n",
            "nt patterns and remittance flows . big_data architectures have been gaining mo\n",
            "wever , architectures that process big_data involve many different components \n",
            "nce of common anti-patterns across big_data architectures and exploiting softw\n",
            " or become second class citizens . big_data is the lifeblood of ai and quantum\n",
            "s to process incredible amounts of big_data . ai is taking over the job ’ s hu\n",
            "ng systems for data warehousing in big_data contexts , mainly organizing data \n",
            "nce of several storage systems for big_data warehousing . however , few of the\n",
            "torage technology for implementing big_data warehousing systems . therefore , \n",
            "ate the advantages of implementing big_data warehouses based on denormalized m\n",
            "easy to intelligently diagnose for big_data processing , because the learning \n",
            " over time move toward becoming as big_data . the traditional model of big_dat\n",
            "ig_data . the traditional model of big_data does not specify any level for cap\n",
            " data . however , when it comes to big_data , such as landsat imagery , such m\n",
            "nce interpretation , ease of use , big_data application , and generalization t\n",
            "networks , and that the effects of big_data are rarely considered . several tr\n",
            "ses machine_learning techniques on big_data platform and builds a new way of f\n",
            "erformance computing cluster based big_data readmission risk analysis framewor\n",
            "the over-all evaluation time using big_data and a parallel computing platform \n",
            "ce . critical systems that produce big_data streams can require human operator\n",
            "ly on medicare , utilizing three ‘ big_data ’ medicare claims datasets with re\n",
            "methods provide a higher speed for big_data classification along with being mo\n",
            "ore compact and interpretable . as big_data processing often takes a long time\n",
            "iety is one of the key features of big_data which causes various parts of data\n",
            "sting . the model applied by using big_data technology and achieved 85.6 % acc\n",
            "arge number of variant variables ( big_data case ) . deep_learning is an incre\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQbUgnGwU6xr"
      },
      "source": [
        "**7.--** Limpie el texto de los abstracts dejando únicamente sustantivos, adjetivos, verbos y adverbios. Debe llevar los verbos a su infinitivo, usar lematización, etc. Tenga en cuenta que en el contexto de los textos analizados, por ejemplo, la frase 'this paper presents' (u otras similares) es muy común; sin embargo, el sustativo paper o el verbo to present no son significativos en el texto, y por tanto no deben ser tenidos en cuenta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAzAuD7DU6xr",
        "outputId": "5025186b-b815-4f64-cf40-dc37ee949c35"
      },
      "source": [
        "# se crea una lista de bigramas para buscar textos frecuentes\n",
        "# no significativos como 'paper presents'. Se analizan \n",
        "# manualmente los primeros 100 textos frecuentes\n",
        "bigramas = abstracts.collocation_list(num=100, window_size=3)\n",
        "\n",
        "# une las tuplas que conforman el bigrama\n",
        "bigramas = [' '.join(text) for text in bigramas]\n",
        "bigramas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['results show',\n",
              " 'experimental results',\n",
              " 'case study',\n",
              " 'paper presents',\n",
              " 'paper proposes',\n",
              " 'recent years',\n",
              " 'proposed method',\n",
              " 'association_rule mining',\n",
              " 'wide range',\n",
              " 'data management',\n",
              " 'different types',\n",
              " 'built environment',\n",
              " 'web pages',\n",
              " 'data sharing',\n",
              " 'decision makers',\n",
              " 'decision tree',\n",
              " 'social network',\n",
              " 'class imbalance',\n",
              " 'results indicate',\n",
              " 'kmo hourly',\n",
              " 'data_mining techniques',\n",
              " 'real estate',\n",
              " 'random forest',\n",
              " 'remote sensing',\n",
              " 'results suggest',\n",
              " 'experimental show',\n",
              " 'proposed algorithm',\n",
              " 'widely used',\n",
              " 'urban form',\n",
              " 'stock market',\n",
              " 'records kmo',\n",
              " 'hourly records',\n",
              " 'proposed approach',\n",
              " 'large number',\n",
              " 'knowledge discovery',\n",
              " 'heat islands',\n",
              " 'publicly available',\n",
              " 'paper discusses',\n",
              " 'careless respondents',\n",
              " 'new york',\n",
              " 'support vector',\n",
              " 'population density',\n",
              " 'climate change',\n",
              " 'logistic regression',\n",
              " 'shed light',\n",
              " 'large datasets',\n",
              " 'human mobility',\n",
              " 'important role',\n",
              " 'statistically significant',\n",
              " 'abstract available',\n",
              " 'large amounts',\n",
              " 'data set',\n",
              " 'propose new',\n",
              " 'preferential attachment',\n",
              " 'years.0 years.0',\n",
              " 'data sources',\n",
              " 'propose novel',\n",
              " 'paper describes',\n",
              " 'monte carlo',\n",
              " 'gas hydrates',\n",
              " 'gas hydrate',\n",
              " 'extensive experiments',\n",
              " 'pattern mining',\n",
              " 'sentiment analysis',\n",
              " 'even though',\n",
              " 'urban sprawl',\n",
              " 'data science',\n",
              " 'high dimensional',\n",
              " 'road accident',\n",
              " 'vector machine',\n",
              " 'natural language',\n",
              " 'default probability',\n",
              " 'space weather',\n",
              " 'different levels',\n",
              " 'better understanding',\n",
              " 'google trends',\n",
              " 'dimension reduction',\n",
              " 'urban planning',\n",
              " 'social networking',\n",
              " 'large volumes',\n",
              " 'scientific data',\n",
              " 'space syntax',\n",
              " 'around world',\n",
              " 'national institute',\n",
              " 'human behavior',\n",
              " 'intellectual property',\n",
              " 'predictive analytics',\n",
              " 'fuzzy logic',\n",
              " 'many applications',\n",
              " 'intrusion detection',\n",
              " 'case studies',\n",
              " 'taking account',\n",
              " 'naïve bayes',\n",
              " '10almost 10almost',\n",
              " 'future research',\n",
              " 'developing countries',\n",
              " 'results demonstrate',\n",
              " 'genetic algorithm',\n",
              " 'road accidents',\n",
              " 'recommender system']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phy7oCGsU6xs"
      },
      "source": [
        "# texto no significativo\n",
        "exclusion = [\n",
        "    'results show',\n",
        "    'experimental results',\n",
        "    'case study',\n",
        "    'paper presents',\n",
        "    'paper proposes',\n",
        "    'recent years',\n",
        "    'proposed method',\n",
        "    'wide range',\n",
        "    'different types',\n",
        "    'built environment',\n",
        "    'results indicate',\n",
        "    'results suggest',\n",
        "    'experimental show',\n",
        "    'proposed algorithm',\n",
        "    'widely used',\n",
        "    'proposed approach',\n",
        "    'knowledge discovery',\n",
        "    'heat islands',\n",
        "    'publicly available',\n",
        "    'paper discusses',\n",
        "    'statistically significant',\n",
        "    'propose new',\n",
        "    'paper describes',\n",
        "    'case studies',\n",
        "    'results demonstrate',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w51vktbKU6xs"
      },
      "source": [
        "# remueve el texto no significativo\n",
        "# se incoporan los resultados al preprocesamiento\n",
        "\n",
        "# Se agrega el paso adicional\n",
        "abstracts = data.Abstract.copy()\n",
        "abstracts = abstracts.dropna()\n",
        "abstracts = abstracts.map(lambda w: w.strip())\n",
        "abstracts = abstracts.map(lambda w: w + \".\" if w[-1] != \".\" else w)\n",
        "abstracts = abstracts.map(lambda w: w.lower(), na_action=\"ignore\")\n",
        "abstracts = abstracts.tolist()\n",
        "abstracts = \" \".join(abstracts)\n",
        "\n",
        "# se reemplaza el bigrama original por el bigrama con '_'\n",
        "for bigrama in bigramas:\n",
        "    abstracts = abstracts.replace(bigrama.replace('_', ' '), bigrama)\n",
        "\n",
        "# borra el texto no significativo\n",
        "for texto in exclusion:\n",
        "    abstracts = abstracts.replace(texto, '')\n",
        "    \n",
        "from nltk.tokenize import word_tokenize\n",
        "abstracts = word_tokenize(abstracts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aah324CU6xt",
        "outputId": "da108fa8-582f-461b-ead1-f480b9d3876c"
      },
      "source": [
        "# Lemmatization.\n",
        "# ejemplo en https://jdvelasq.github.io/mineria-de-datos/notebooks/text-analytics/3-06-NLTK-transformaciones-basicas.html\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# texto taggeado\n",
        "texto_taggeado = nltk.pos_tag(abstracts)\n",
        "\n",
        "nuevo_texto = []\n",
        "for word, tag in texto_taggeado:\n",
        "    if tag[:2] == 'VB':\n",
        "        # verbo\n",
        "        nuevo_texto += [wnl.lemmatize(word, wordnet.VERB)]\n",
        "    elif tag[:2] == 'NN':\n",
        "        # sustantivo\n",
        "        nuevo_texto += [wnl.lemmatize(word, wordnet.NOUN)]\n",
        "    elif tag[:2] == 'JJ':\n",
        "        # adjetivos\n",
        "        nuevo_texto += [wnl.lemmatize(word, wordnet.ADJ)]\n",
        "    elif tag[:2] == 'RB':\n",
        "        # adverbios\n",
        "        nuevo_texto += [wnl.lemmatize(word, wordnet.ADV)]\n",
        "    elif '_' in word:\n",
        "        # es una collocation\n",
        "        nuevo_texto += [word]\n",
        "    else:\n",
        "        # no hace nada\n",
        "        pass\n",
        "\n",
        "# convierte el texto a NLTK y lo imprime el texto limpiado\n",
        "nuevo_texto = nltk.Text(nuevo_texto)\n",
        "nuevo_texto"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Text: mobility be fundamental requirement human life significant societal...>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKy0F_LXU6xu"
      },
      "source": [
        "**8.--** Imprima la cantidad de verbos, adverbios, sustantivos y adjetivos del texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjeb5B8PU6xv",
        "outputId": "149a694d-05be-412b-843b-2d24df1d1c35"
      },
      "source": [
        "# Crea un diccionario para realiar el conteo e inicia \n",
        "# los contadores en cero tal como se hizo para TextBlob\n",
        "counter_dict = {}\n",
        "counter_dict['NN'] = 0\n",
        "counter_dict['JJ'] = 0\n",
        "counter_dict['VB'] = 0\n",
        "counter_dict['RB'] = 0\n",
        "\n",
        "# Recorre las parejas de palabras y tags y cuenta\n",
        "# usando los tipos de tags\n",
        "texto_taggeado = nltk.pos_tag(nuevo_texto)\n",
        "for word, tag in texto_taggeado:\n",
        "    if tag[0:2] == 'NN':\n",
        "        # es un sustantivo\n",
        "        counter_dict['NN'] += 1\n",
        "        \n",
        "    if tag[:2] == 'JJ':\n",
        "        # es un adjetivo\n",
        "        counter_dict['JJ'] += 1\n",
        "        \n",
        "    if tag[:2] == 'VB':\n",
        "        # es un verbo\n",
        "        counter_dict['VB'] += 1\n",
        "        \n",
        "    if tag[:2] == 'RB':\n",
        "        # es un adverbio\n",
        "        counter_dict['RB'] += 1\n",
        "        \n",
        "for key in counter_dict.keys():\n",
        "    print(key, counter_dict[key])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN 107539\n",
            "JJ 52678\n",
            "VB 29820\n",
            "RB 10756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO-f3E4uU6xv"
      },
      "source": [
        "**9.--** Imprima la lista ordenada por importancia de las palabras significativas (verbos, adverbios, sustantivos y adjetivos) en el texto transformado. Para computar la importancia use el modelo TFIDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj9l6MebU6xw",
        "outputId": "d29842e1-e64b-4572-d2e3-8a3a404885d7"
      },
      "source": [
        "# El proceso de transformación es igual al explicado en clase, donde cada abstract es un documento.\n",
        "# El modelo TFIDF es explicado en \n",
        "# https://jdvelasq.github.io/mineria-de-datos/notebooks/text-analytics/5-30-esquemas-de-representacion.html\n",
        "\n",
        "# el codigo es igual al usado en clase\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "tokenizer = nltk.WordPunctTokenizer()\n",
        "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(document):\n",
        "    document = re.sub(r'[^a-zA-Z\\s]', '', document, re.I|re.A)\n",
        "    document = document.lower().strip()\n",
        "    tokens = tokenizer.tokenize(document)\n",
        "    tokens = [token for token in tokens if token not in STOPWORDS]\n",
        "    document = ' '.join(tokens)\n",
        "    return document\n",
        "\n",
        "data['Normalized_Abstract'] = data.Abstract.map(normalize_document)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DOI</th>\n",
              "      <th>Link</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Normalized_Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.1140/epjds/s13688-019-0196-6</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>Mobility is one of the fundamental requirement...</td>\n",
              "      <td>mobility one fundamental requirements human li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.1140/epjds/s13688-019-0193-9</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>The recent rise of the political extremism in ...</td>\n",
              "      <td>recent rise political extremism western countr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.1140/epjds/s13688-019-0194-8</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>The power of the press to shape the informatio...</td>\n",
              "      <td>power press shape informational landscape popu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.1140/epjds/s13688-019-0195-7</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>Identifying influential nodes in a network is ...</td>\n",
              "      <td>identifying influential nodes network fundamen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.1140/epjds/s13688-019-0191-y</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>To complement traditional dietary surveys, whi...</td>\n",
              "      <td>complement traditional dietary surveys costly ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1897</th>\n",
              "      <td>10.2481/dsj.2.100</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>In this article, we intend to show how useful ...</td>\n",
              "      <td>article intend show useful exploratory spatial...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1898</th>\n",
              "      <td>10.2481/dsj.2.79</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>In recent geographical information science lit...</td>\n",
              "      <td>recent geographical information science litera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1899</th>\n",
              "      <td>10.2481/dsj.2.90</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>The fact that many decisions need a combinatio...</td>\n",
              "      <td>fact many decisions need combination informati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1900</th>\n",
              "      <td>NaN</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>The report from Woolmark Business Intelligence...</td>\n",
              "      <td>report woolmark business intelligence consider...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1901</th>\n",
              "      <td>NaN</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>Changing consumer lifestyles and increased tim...</td>\n",
              "      <td>changing consumer lifestyles increased time pr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1902 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  DOI  \\\n",
              "0     10.1140/epjds/s13688-019-0196-6   \n",
              "1     10.1140/epjds/s13688-019-0193-9   \n",
              "2     10.1140/epjds/s13688-019-0194-8   \n",
              "3     10.1140/epjds/s13688-019-0195-7   \n",
              "4     10.1140/epjds/s13688-019-0191-y   \n",
              "...                               ...   \n",
              "1897                10.2481/dsj.2.100   \n",
              "1898                 10.2481/dsj.2.79   \n",
              "1899                 10.2481/dsj.2.90   \n",
              "1900                              NaN   \n",
              "1901                              NaN   \n",
              "\n",
              "                                                   Link  \\\n",
              "0     https://www.scopus.com/inward/record.uri?eid=2...   \n",
              "1     https://www.scopus.com/inward/record.uri?eid=2...   \n",
              "2     https://www.scopus.com/inward/record.uri?eid=2...   \n",
              "3     https://www.scopus.com/inward/record.uri?eid=2...   \n",
              "4     https://www.scopus.com/inward/record.uri?eid=2...   \n",
              "...                                                 ...   \n",
              "1897  https://www.scopus.com/inward/record.uri?eid=2...   \n",
              "1898  https://www.scopus.com/inward/record.uri?eid=2...   \n",
              "1899  https://www.scopus.com/inward/record.uri?eid=2...   \n",
              "1900  https://www.scopus.com/inward/record.uri?eid=2...   \n",
              "1901  https://www.scopus.com/inward/record.uri?eid=2...   \n",
              "\n",
              "                                               Abstract  \\\n",
              "0     Mobility is one of the fundamental requirement...   \n",
              "1     The recent rise of the political extremism in ...   \n",
              "2     The power of the press to shape the informatio...   \n",
              "3     Identifying influential nodes in a network is ...   \n",
              "4     To complement traditional dietary surveys, whi...   \n",
              "...                                                 ...   \n",
              "1897  In this article, we intend to show how useful ...   \n",
              "1898  In recent geographical information science lit...   \n",
              "1899  The fact that many decisions need a combinatio...   \n",
              "1900  The report from Woolmark Business Intelligence...   \n",
              "1901  Changing consumer lifestyles and increased tim...   \n",
              "\n",
              "                                    Normalized_Abstract  \n",
              "0     mobility one fundamental requirements human li...  \n",
              "1     recent rise political extremism western countr...  \n",
              "2     power press shape informational landscape popu...  \n",
              "3     identifying influential nodes network fundamen...  \n",
              "4     complement traditional dietary surveys costly ...  \n",
              "...                                                 ...  \n",
              "1897  article intend show useful exploratory spatial...  \n",
              "1898  recent geographical information science litera...  \n",
              "1899  fact many decisions need combination informati...  \n",
              "1900  report woolmark business intelligence consider...  \n",
              "1901  changing consumer lifestyles increased time pr...  \n",
              "\n",
              "[1902 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6p4POOYU6xx",
        "outputId": "58750290-3ca1-45b6-b65e-d3e69d48a50c"
      },
      "source": [
        "# se usa pos_tag como en los puntos anteriores\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "data['Normalized_Abstract'] = data.Normalized_Abstract.map(word_tokenize)\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# teggea cada palabra de cada abstract\n",
        "wnl = WordNetLemmatizer()\n",
        "data['Tagged_Abstract'] = data.Normalized_Abstract.map(nltk.pos_tag)\n",
        "\n",
        "def f(texto_taggeado):\n",
        "    resultado = []\n",
        "    for word, tag in texto_taggeado:\n",
        "        if tag[:2] == 'VB':\n",
        "            # verbo\n",
        "            resultado += [wnl.lemmatize(word, wordnet.VERB)]\n",
        "        elif tag[:2] == 'NN':\n",
        "            # sustantivo\n",
        "            resultado += [wnl.lemmatize(word, wordnet.NOUN)]\n",
        "        elif tag[:2] == 'JJ':\n",
        "            # adjetivos\n",
        "            resultado += [wnl.lemmatize(word, wordnet.ADJ)]\n",
        "        elif tag[:2] == 'RB':\n",
        "            # adverbios\n",
        "            resultado += [wnl.lemmatize(word, wordnet.ADV)]\n",
        "        elif '_' in word:\n",
        "            # es una collocation\n",
        "            resultado += [word]\n",
        "        else:\n",
        "            # no hace nada\n",
        "            pass\n",
        "    return ' '.join(resultado)\n",
        "\n",
        "data['Abstract_listo'] = data.Tagged_Abstract.map(f)\n",
        "\n",
        "data.Abstract_listo.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    mobility fundamental requirement human life si...\n",
              "1    recent rise political extremism western countr...\n",
              "2    power press shape informational landscape popu...\n",
              "3    identify influential node network fundamental ...\n",
              "4    complement traditional dietary survey costly l...\n",
              "Name: Abstract_listo, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yQaIxcCU6xx",
        "outputId": "5ae47119-d052-4c43-c88d-a41b18d7e2ab"
      },
      "source": [
        "# Computa la matriz TFIDF igual que en clase.\n",
        "# https://jdvelasq.github.io/mineria-de-datos/notebooks/text-analytics/5-30-esquemas-de-representacion.html\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "cv = CountVectorizer(min_df=0., max_df=1.)\n",
        "cv_matrix = cv.fit_transform(data.Abstract_listo.tolist())\n",
        "cv_matrix = cv_matrix.toarray()\n",
        "\n",
        "\n",
        "tt = TfidfTransformer(norm='l2', use_idf=True)\n",
        "tt_matrix = tt.fit_transform(cv_matrix)\n",
        "tt_matrix = tt_matrix.toarray()\n",
        "vocab = cv.get_feature_names()\n",
        "tfidf = pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)\n",
        "\n",
        "tfidf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aa</th>\n",
              "      <th>aakers</th>\n",
              "      <th>aarp</th>\n",
              "      <th>aas</th>\n",
              "      <th>ab</th>\n",
              "      <th>abandon</th>\n",
              "      <th>abatement</th>\n",
              "      <th>abbm</th>\n",
              "      <th>abbreviate</th>\n",
              "      <th>abc</th>\n",
              "      <th>...</th>\n",
              "      <th>zhuzhou</th>\n",
              "      <th>zimbabwe</th>\n",
              "      <th>zip</th>\n",
              "      <th>zlotnicki</th>\n",
              "      <th>zmp</th>\n",
              "      <th>zonation</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoning</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1897</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1898</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1899</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1900</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1901</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1902 rows × 13644 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       aa  aakers  aarp  aas   ab  abandon  abatement  abbm  abbreviate  abc  \\\n",
              "0     0.0     0.0   0.0  0.0  0.0      0.0        0.0   0.0         0.0  0.0   \n",
              "1     0.0     0.0   0.0  0.0  0.0      0.0        0.0   0.0         0.0  0.0   \n",
              "2     0.0     0.0   0.0  0.0  0.0      0.0        0.0   0.0         0.0  0.0   \n",
              "3     0.0     0.0   0.0  0.0  0.0      0.0        0.0   0.0         0.0  0.0   \n",
              "4     0.0     0.0   0.0  0.0  0.0      0.0        0.0   0.0         0.0  0.0   \n",
              "...   ...     ...   ...  ...  ...      ...        ...   ...         ...  ...   \n",
              "1897  0.0     0.0   0.0  0.0  0.0      0.0        0.0   0.0         0.0  0.0   \n",
              "1898  0.0     0.0   0.0  0.0  0.0      0.0        0.0   0.0         0.0  0.0   \n",
              "1899  0.0     0.0   0.0  0.0  0.0      0.0        0.0   0.0         0.0  0.0   \n",
              "1900  0.0     0.0   0.0  0.0  0.0      0.0        0.0   0.0         0.0  0.0   \n",
              "1901  0.0     0.0   0.0  0.0  0.0      0.0        0.0   0.0         0.0  0.0   \n",
              "\n",
              "      ...  zhuzhou  zimbabwe  zip  zlotnicki  zmp  zonation  zone  zoning  \\\n",
              "0     ...      0.0       0.0  0.0        0.0  0.0       0.0   0.0     0.0   \n",
              "1     ...      0.0       0.0  0.0        0.0  0.0       0.0   0.0     0.0   \n",
              "2     ...      0.0       0.0  0.0        0.0  0.0       0.0   0.0     0.0   \n",
              "3     ...      0.0       0.0  0.0        0.0  0.0       0.0   0.0     0.0   \n",
              "4     ...      0.0       0.0  0.0        0.0  0.0       0.0   0.0     0.0   \n",
              "...   ...      ...       ...  ...        ...  ...       ...   ...     ...   \n",
              "1897  ...      0.0       0.0  0.0        0.0  0.0       0.0   0.0     0.0   \n",
              "1898  ...      0.0       0.0  0.0        0.0  0.0       0.0   0.0     0.0   \n",
              "1899  ...      0.0       0.0  0.0        0.0  0.0       0.0   0.0     0.0   \n",
              "1900  ...      0.0       0.0  0.0        0.0  0.0       0.0   0.0     0.0   \n",
              "1901  ...      0.0       0.0  0.0        0.0  0.0       0.0   0.0     0.0   \n",
              "\n",
              "      zoom   zp  \n",
              "0      0.0  0.0  \n",
              "1      0.0  0.0  \n",
              "2      0.0  0.0  \n",
              "3      0.0  0.0  \n",
              "4      0.0  0.0  \n",
              "...    ...  ...  \n",
              "1897   0.0  0.0  \n",
              "1898   0.0  0.0  \n",
              "1899   0.0  0.0  \n",
              "1900   0.0  0.0  \n",
              "1901   0.0  0.0  \n",
              "\n",
              "[1902 rows x 13644 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpjcqrrqU6xx",
        "outputId": "b2d93c24-903c-47ea-ee56-c33e28dd17f8"
      },
      "source": [
        "# la importancia se calcula como la suma de la calumna para cada palabra\n",
        "importancia = tfidf.sum(axis=0)\n",
        "\n",
        "# ordena el vector de mayor a menor que es lo que se pide en el punto\n",
        "importancia.sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "data                     115.11\n",
              "use                       51.86\n",
              "model                     49.18\n",
              "method                    39.91\n",
              "system                    39.72\n",
              "                          ...  \n",
              "removed                    0.03\n",
              "rsquared                   0.03\n",
              "continuouscategorical      0.03\n",
              "multicollinearity          0.03\n",
              "continuouscontinuous       0.03\n",
              "Length: 13644, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeGyhilpU6xy"
      },
      "source": [
        "**10.--** Compute la matrix de similaridad entre abstracts y determine la cantidad óptima de clusters. Esta técnica permite detectar los temas dominantes en los trabajos. Puede interpretar que tema representa cada cluster?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1h87mBTU6xy",
        "outputId": "dc2bf406-009a-4370-d865-19617008e1f6"
      },
      "source": [
        "# Para computar la similitud de documentos se usa la matriz tfidf y la \n",
        "# medida de similitud de coseno, ejemplo presentado en\n",
        "# https://jdvelasq.github.io/mineria-de-datos/notebooks/text-analytics/5-30-esquemas-de-representacion.html\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(tfidf)\n",
        "similarity_df = pd.DataFrame(similarity_matrix)\n",
        "similarity_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1892</th>\n",
              "      <th>1893</th>\n",
              "      <th>1894</th>\n",
              "      <th>1895</th>\n",
              "      <th>1896</th>\n",
              "      <th>1897</th>\n",
              "      <th>1898</th>\n",
              "      <th>1899</th>\n",
              "      <th>1900</th>\n",
              "      <th>1901</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.007858</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>0.021786</td>\n",
              "      <td>0.017457</td>\n",
              "      <td>0.006513</td>\n",
              "      <td>0.016147</td>\n",
              "      <td>0.052501</td>\n",
              "      <td>0.038402</td>\n",
              "      <td>0.025479</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013238</td>\n",
              "      <td>0.023799</td>\n",
              "      <td>0.012789</td>\n",
              "      <td>0.028224</td>\n",
              "      <td>0.023635</td>\n",
              "      <td>0.022395</td>\n",
              "      <td>0.010239</td>\n",
              "      <td>0.014668</td>\n",
              "      <td>0.009202</td>\n",
              "      <td>0.018221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.007858</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.029818</td>\n",
              "      <td>0.012900</td>\n",
              "      <td>0.016669</td>\n",
              "      <td>0.004912</td>\n",
              "      <td>0.021143</td>\n",
              "      <td>0.022428</td>\n",
              "      <td>0.006057</td>\n",
              "      <td>0.012150</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014042</td>\n",
              "      <td>0.022012</td>\n",
              "      <td>0.013496</td>\n",
              "      <td>0.007458</td>\n",
              "      <td>0.009042</td>\n",
              "      <td>0.009362</td>\n",
              "      <td>0.007659</td>\n",
              "      <td>0.024561</td>\n",
              "      <td>0.010308</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.014000</td>\n",
              "      <td>0.029818</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.030061</td>\n",
              "      <td>0.021376</td>\n",
              "      <td>0.004536</td>\n",
              "      <td>0.024867</td>\n",
              "      <td>0.008719</td>\n",
              "      <td>0.019267</td>\n",
              "      <td>0.016821</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017223</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011358</td>\n",
              "      <td>0.007598</td>\n",
              "      <td>0.066635</td>\n",
              "      <td>0.007810</td>\n",
              "      <td>0.025502</td>\n",
              "      <td>0.002108</td>\n",
              "      <td>0.013584</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.021786</td>\n",
              "      <td>0.012900</td>\n",
              "      <td>0.030061</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.012837</td>\n",
              "      <td>0.010476</td>\n",
              "      <td>0.040884</td>\n",
              "      <td>0.032250</td>\n",
              "      <td>0.147350</td>\n",
              "      <td>0.048429</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028119</td>\n",
              "      <td>0.049060</td>\n",
              "      <td>0.020190</td>\n",
              "      <td>0.007693</td>\n",
              "      <td>0.024260</td>\n",
              "      <td>0.029820</td>\n",
              "      <td>0.039577</td>\n",
              "      <td>0.022872</td>\n",
              "      <td>0.015888</td>\n",
              "      <td>0.003221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.017457</td>\n",
              "      <td>0.016669</td>\n",
              "      <td>0.021376</td>\n",
              "      <td>0.012837</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.018803</td>\n",
              "      <td>0.017504</td>\n",
              "      <td>0.049988</td>\n",
              "      <td>0.011452</td>\n",
              "      <td>0.044946</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016082</td>\n",
              "      <td>0.046262</td>\n",
              "      <td>0.005013</td>\n",
              "      <td>0.003590</td>\n",
              "      <td>0.034083</td>\n",
              "      <td>0.017680</td>\n",
              "      <td>0.012271</td>\n",
              "      <td>0.008612</td>\n",
              "      <td>0.032221</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1897</th>\n",
              "      <td>0.022395</td>\n",
              "      <td>0.009362</td>\n",
              "      <td>0.007810</td>\n",
              "      <td>0.029820</td>\n",
              "      <td>0.017680</td>\n",
              "      <td>0.003712</td>\n",
              "      <td>0.010778</td>\n",
              "      <td>0.012170</td>\n",
              "      <td>0.024244</td>\n",
              "      <td>0.020131</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031294</td>\n",
              "      <td>0.070789</td>\n",
              "      <td>0.038416</td>\n",
              "      <td>0.233146</td>\n",
              "      <td>0.061152</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.348679</td>\n",
              "      <td>0.125587</td>\n",
              "      <td>0.005108</td>\n",
              "      <td>0.006716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1898</th>\n",
              "      <td>0.010239</td>\n",
              "      <td>0.007659</td>\n",
              "      <td>0.025502</td>\n",
              "      <td>0.039577</td>\n",
              "      <td>0.012271</td>\n",
              "      <td>0.012825</td>\n",
              "      <td>0.022627</td>\n",
              "      <td>0.010660</td>\n",
              "      <td>0.016870</td>\n",
              "      <td>0.005972</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025581</td>\n",
              "      <td>0.024600</td>\n",
              "      <td>0.050857</td>\n",
              "      <td>0.246273</td>\n",
              "      <td>0.083420</td>\n",
              "      <td>0.348679</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.129722</td>\n",
              "      <td>0.029208</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1899</th>\n",
              "      <td>0.014668</td>\n",
              "      <td>0.024561</td>\n",
              "      <td>0.002108</td>\n",
              "      <td>0.022872</td>\n",
              "      <td>0.008612</td>\n",
              "      <td>0.004224</td>\n",
              "      <td>0.006503</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033568</td>\n",
              "      <td>0.025975</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055350</td>\n",
              "      <td>0.038043</td>\n",
              "      <td>0.014743</td>\n",
              "      <td>0.141654</td>\n",
              "      <td>0.031400</td>\n",
              "      <td>0.125587</td>\n",
              "      <td>0.129722</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.030020</td>\n",
              "      <td>0.047835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1900</th>\n",
              "      <td>0.009202</td>\n",
              "      <td>0.010308</td>\n",
              "      <td>0.013584</td>\n",
              "      <td>0.015888</td>\n",
              "      <td>0.032221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010731</td>\n",
              "      <td>0.012229</td>\n",
              "      <td>0.003994</td>\n",
              "      <td>0.005608</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028443</td>\n",
              "      <td>0.039241</td>\n",
              "      <td>0.010154</td>\n",
              "      <td>0.021999</td>\n",
              "      <td>0.016487</td>\n",
              "      <td>0.005108</td>\n",
              "      <td>0.029208</td>\n",
              "      <td>0.030020</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.296867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1901</th>\n",
              "      <td>0.018221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017154</td>\n",
              "      <td>0.002008</td>\n",
              "      <td>0.013946</td>\n",
              "      <td>0.010294</td>\n",
              "      <td>0.001002</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008821</td>\n",
              "      <td>0.006043</td>\n",
              "      <td>0.012075</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006716</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.047835</td>\n",
              "      <td>0.296867</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1902 rows × 1902 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5         6     \\\n",
              "0     1.000000  0.007858  0.014000  0.021786  0.017457  0.006513  0.016147   \n",
              "1     0.007858  1.000000  0.029818  0.012900  0.016669  0.004912  0.021143   \n",
              "2     0.014000  0.029818  1.000000  0.030061  0.021376  0.004536  0.024867   \n",
              "3     0.021786  0.012900  0.030061  1.000000  0.012837  0.010476  0.040884   \n",
              "4     0.017457  0.016669  0.021376  0.012837  1.000000  0.018803  0.017504   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1897  0.022395  0.009362  0.007810  0.029820  0.017680  0.003712  0.010778   \n",
              "1898  0.010239  0.007659  0.025502  0.039577  0.012271  0.012825  0.022627   \n",
              "1899  0.014668  0.024561  0.002108  0.022872  0.008612  0.004224  0.006503   \n",
              "1900  0.009202  0.010308  0.013584  0.015888  0.032221  0.000000  0.010731   \n",
              "1901  0.018221  0.000000  0.000000  0.003221  0.000000  0.017154  0.002008   \n",
              "\n",
              "          7         8         9     ...      1892      1893      1894  \\\n",
              "0     0.052501  0.038402  0.025479  ...  0.013238  0.023799  0.012789   \n",
              "1     0.022428  0.006057  0.012150  ...  0.014042  0.022012  0.013496   \n",
              "2     0.008719  0.019267  0.016821  ...  0.017223  0.000000  0.011358   \n",
              "3     0.032250  0.147350  0.048429  ...  0.028119  0.049060  0.020190   \n",
              "4     0.049988  0.011452  0.044946  ...  0.016082  0.046262  0.005013   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "1897  0.012170  0.024244  0.020131  ...  0.031294  0.070789  0.038416   \n",
              "1898  0.010660  0.016870  0.005972  ...  0.025581  0.024600  0.050857   \n",
              "1899  0.000000  0.033568  0.025975  ...  0.055350  0.038043  0.014743   \n",
              "1900  0.012229  0.003994  0.005608  ...  0.028443  0.039241  0.010154   \n",
              "1901  0.013946  0.010294  0.001002  ...  0.008821  0.006043  0.012075   \n",
              "\n",
              "          1895      1896      1897      1898      1899      1900      1901  \n",
              "0     0.028224  0.023635  0.022395  0.010239  0.014668  0.009202  0.018221  \n",
              "1     0.007458  0.009042  0.009362  0.007659  0.024561  0.010308  0.000000  \n",
              "2     0.007598  0.066635  0.007810  0.025502  0.002108  0.013584  0.000000  \n",
              "3     0.007693  0.024260  0.029820  0.039577  0.022872  0.015888  0.003221  \n",
              "4     0.003590  0.034083  0.017680  0.012271  0.008612  0.032221  0.000000  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "1897  0.233146  0.061152  1.000000  0.348679  0.125587  0.005108  0.006716  \n",
              "1898  0.246273  0.083420  0.348679  1.000000  0.129722  0.029208  0.000000  \n",
              "1899  0.141654  0.031400  0.125587  0.129722  1.000000  0.030020  0.047835  \n",
              "1900  0.021999  0.016487  0.005108  0.029208  0.030020  1.000000  0.296867  \n",
              "1901  0.000000  0.000000  0.006716  0.000000  0.047835  0.296867  1.000000  \n",
              "\n",
              "[1902 rows x 1902 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4qWRjZ8U6xz"
      },
      "source": [
        "# Para calcular los clusters se usa la función fcluster. El método entrega \n",
        "# la cantidad de clusters\n",
        "\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "Z = linkage(similarity_matrix, \"ward\")\n",
        "\n",
        "max_dist = 1.0\n",
        "cluster_labels = fcluster(Z, max_dist, criterion='distance')\n",
        "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
        "\n",
        "data['Cluster'] = cluster_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRl0B9z5U6xz",
        "outputId": "c54755a6-0ff9-4a57-a041-33f9f154d8b9"
      },
      "source": [
        "# numero de clusters\n",
        "cluster_labels.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClusterLabel    1854\n",
              "dtype: int32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzfrZGGWU6x0",
        "outputId": "dafdeb13-4935-4a79-eb45-adb8f870a328"
      },
      "source": [
        "# numero de documentos\n",
        "len(cluster_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1902"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqAWjYz4U6x1"
      },
      "source": [
        "Para interpretar el tema de cada cluster se puede sacar las palabras claves más importantes de cada abstract."
      ]
    }
  ]
}